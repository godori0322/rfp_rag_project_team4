# document.py

import os
import re
import pandas as pd
import pdfplumber
import numpy as np
from typing import List, Dict, Any
import string
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

from config import Config


def convert_table_to_markdown(table: List[List[str]]) -> str:
    """PDFplumberë¡œ ì¶”ì¶œëœ í…Œì´ë¸”(list of lists)ì„ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    markdown_table = ""
    if not table:
        return ""

    # í—¤ë” ìƒì„±
    header = [str(cell) if cell is not None else "" for cell in table[0]]
    markdown_table += "| " + " | ".join(header) + " |\n"
    
    # êµ¬ë¶„ì„  ìƒì„±
    markdown_table += "| " + " | ".join(["---"] * len(header)) + " |\n"
    
    # ë³¸ë¬¸ ìƒì„±
    for row in table[1:]:
        body = [str(cell) if cell is not None else "" for cell in row]
        markdown_table += "| " + " | ".join(body) + " |\n"
        
    return markdown_table

def clean_text_with_regex(text: str, patterns: List[str]) -> str:
    """ì£¼ì–´ì§„ ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´ë“¤ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì²­ì†Œí•©ë‹ˆë‹¤."""
    for pattern in patterns:
        text = re.sub(pattern, "", text)
        # Normalize whitespace: strip lines, replace multiple spaces/tabs, reduce newlines
        # 1. Strip leading/trailing spaces from each line
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        # 2. Replace multiple spaces/tabs with a single space
        text = re.sub(r'[ \t]+', ' ', text)
        # 3. Reduce multiple newlines to a single newline
        text = re.sub(r'\n{2,}', '\n', text)
    return text


def load_documents():
    """CSV ë©”íƒ€ë°ì´í„°ì™€ PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜."""
    def ext(original_filename, ext='pdf'):
        base_filename, _ = os.path.splitext(original_filename)
        return f"{base_filename}.{ext}"

    df = pd.read_csv(os.path.join(Config.DATA_PATH, "data_list.csv"))
    
    date_columns = ['ê³µê°œ ì¼ì', 'ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼', 'ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼']
    for col in date_columns:
        df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%Y%m%d').fillna(0).astype(int)

    df = df.fillna('')

    all_docs = []
    for index, row in df.iterrows():
        print(f"[{index + 1}/{len(df)}] ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: {row['ì‚¬ì—…ëª…']}")
        metadata = {
            'rfp_number': row['ê³µê³  ë²ˆí˜¸'],
            'project_title': row['ì‚¬ì—…ëª…'],
            'budget_krw': row['ì‚¬ì—… ê¸ˆì•¡'],
            'agency': row['ë°œì£¼ ê¸°ê´€'],
            'publish_date': row['ê³µê°œ ì¼ì'],
            'bid_start_date': row['ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼'],
            'bid_end_date': row['ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼'],
            'summary': row['ì‚¬ì—… ìš”ì•½'],
            'filename': row['íŒŒì¼ëª…']
        }
        
        filepath = os.path.join(Config.PDF_PATH, ext(row['íŒŒì¼ëª…']))
        
        # ğŸ’¡ ê°œì„ ëœ chunk í•¨ìˆ˜ í˜¸ì¶œ
        docs = chunk(
            filepath=filepath, 
            metadata=metadata,
            header_percentile=90, # ìƒìœ„ 10% í°íŠ¸ í¬ê¸°ë¥¼ í—¤ë”ë¡œ ê°„ì£¼
            final_chunk_size=1000, # ì²­í¬ ì‚¬ì´ì¦ˆ ì‹¤í—˜
            final_chunk_overlap=120  # ì²­í¬ ì˜¤ë²„ë© ì‹¤í—˜
        )
        all_docs.append(docs)

    print(f"ì´ {len(df)}ê°œì˜ ì›ë³¸ ë¬¸ì„œì—ì„œ {len(all_docs)}ê°œì˜ ì²­í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    return all_docs


def chunk(filepath: str, 
          metadata: dict, 
          header_percentile: int = 90, 
          final_chunk_size: int = 1000, 
          final_chunk_overlap: int = 120,
          noise_patterns: List[str] = None
         ) -> List[Document]:
    """
    ê°œì„ ëœ ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹ í•¨ìˆ˜ (í…Œì´ë¸” ì¤‘ë³µ ì œê±° í¬í•¨).

    1. (ë…¸ì´ì¦ˆ ì œê±°) ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€ ì œê±°
    2. (í…Œì´ë¸” ì²˜ë¦¬) í…Œì´ë¸”ì„ Markdownìœ¼ë¡œ ë³€í™˜ (ì¤‘ë³µ ë°©ì§€)
    3. (í…ìŠ¤íŠ¸ ì¶”ì¶œ) extract_text() ê¸°ë³¸ ì‚¬ìš©, í…Œì´ë¸” ì˜ì—­ ì œì™¸
    4. (chars ê¸°ë°˜ fallback) í…Œì´ë¸” ì˜ì—­ ì œì™¸
    5. (ë™ì  í—¤ë” íƒì§€) í°íŠ¸ í¬ê¸° ë¶„í¬ ë¶„ì„
    6. 1ì°¨ ì²­í‚¹: í—¤ë” ê¸°ì¤€ ì±•í„° ìƒì„±
    7. 2ì°¨ ì²­í‚¹: RecursiveCharacterTextSplitterë¡œ ë¶„í• 
    """
    if noise_patterns is None:
        noise_patterns = [
            r"^\s*-\s*\d+\s*-\s*$",      # "- 1 -", "- 2 -" (line only)
            r"-\s*\d+\s*-",                # "- 18 -" anywhere in line
            r"^\s*\d+\s*$",                # í˜ì´ì§€ ë²ˆí˜¸ë§Œ ìˆëŠ” ê²½ìš°
            r"(?i)page\s*\d+\s*of\s*\d+" # "Page 1 of 10"
        ]

    page_items = []
    all_font_sizes = []

    with pdfplumber.open(filepath) as pdf:
        for page_num, page in enumerate(pdf.pages):
            
            # --- 1. í…Œì´ë¸” ì¶”ì¶œ ---
            tables = page.find_tables()
            table_bboxes = [t.bbox for t in tables]  # í…Œì´ë¸” ì˜ì—­ bbox ì €ì¥
            for table in tables:
                md_table = convert_table_to_markdown(table.extract())
                page_items.append({
                    'type': 'table',
                    'content': md_table,
                    'top': table.bbox[1],
                    'page': page_num
                })

            # --- 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ (í…Œì´ë¸” ì˜ì—­ ì œì™¸) ---
            text = page.extract_text(x_tolerance=5, y_tolerance=5)
            if text:
                lines = text.split('\n')
                filtered_lines = []

                # ë¼ì¸ë³„ bbox ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ fallbackìš© charsë¡œ y ìœ„ì¹˜ í™•ì¸
                words = page.extract_words()
                line_tops = {}
                for w in words:
                    line_text = w['text']
                    top = round(w['top'])
                    line_tops.setdefault(top, []).append(line_text)

                for top, words_in_line in line_tops.items():
                    # í…Œì´ë¸” bboxì™€ ê²¹ì¹˜ì§€ ì•Šìœ¼ë©´ í¬í•¨
                    if not any(b[1] <= top <= b[3] for b in table_bboxes):
                        line_text = " ".join(words_in_line)
                        filtered_lines.append(line_text)

                # Apply noise cleansing to each line individually
                cleaned_lines = [clean_text_with_regex(line, noise_patterns) for line in filtered_lines]
                cleaned_text = "\n".join([line for line in cleaned_lines if line.strip()])
                if cleaned_text.strip():
                    page_items.append({
                        'type': 'text',
                        'content': cleaned_text,
                        'size': 10,
                        'top': 0,
                        'page': page_num
                    })
            else:
                # --- 3. chars ê¸°ë°˜ fallback (í…Œì´ë¸” ì˜ì—­ ì œì™¸) ---
                chars = page.chars
                non_table_chars = [c for c in chars if not any(
                    b[0] <= c["x0"] <= b[2] and b[1] <= c["top"] <= b[3] for b in table_bboxes
                )]

                tolerance = 5
                current_line, current_top, line_size = "", -1000, 10
                for char in non_table_chars:
                    if abs(current_top - char['top']) > tolerance:
                        if current_line.strip():
                            cleaned_line = clean_text_with_regex(current_line, noise_patterns)
                            if cleaned_line.strip():
                                page_items.append({
                                    'type': 'text',
                                    'content': cleaned_line,
                                    'size': line_size,
                                    'top': current_top,
                                    'page': page_num
                                })
                        current_line = ""
                        current_top = char['top']
                        line_size = char.get('size', 10)
                    current_line += char['text']
                    all_font_sizes.append(char.get('size', 10))

                if current_line.strip():
                    cleaned_line = clean_text_with_regex(current_line, noise_patterns)
                    if cleaned_line.strip():
                        page_items.append({
                            'type': 'text',
                            'content': cleaned_line,
                            'size': line_size,
                            'top': current_top,
                            'page': page_num
                        })

    # --- 4. ë™ì  í—¤ë” ì„ê³„ê°’ ê³„ì‚° ---
    if not all_font_sizes:
        header_font_threshold = 18
    else:
        header_font_threshold = np.percentile(all_font_sizes, header_percentile)
        # í—¤ë”ì™€ ë³¸ë¬¸ì´ ë™ì¼í•œ ê²½ìš° ëŒ€ë¹„
        if header_font_threshold == max(all_font_sizes):
            header_font_threshold = max(all_font_sizes) - 1

    # --- 5. 1ì°¨ ì²­í‚¹ (í—¤ë” ê¸°ì¤€) ---
    page_items.sort(key=lambda x: (x['page'], x['top']))
    font_size_chunks = []
    current_chunk_content = ""
    current_chunk_header = f"ë¬¸ì„œ ì‹œì‘ ({os.path.basename(filepath)})"

    for item in page_items:
        if item['type'] == 'text':
            font_size = round(item.get('size', 0))
            text = item['content']

            if font_size >= header_font_threshold:
                if current_chunk_content.strip():
                    font_size_chunks.append({
                        "header": current_chunk_header,
                        "content": current_chunk_content.strip()
                    })
                current_chunk_header = text
                current_chunk_content = ""
            else:
                current_chunk_content += text + "\n"

        elif item['type'] == 'table':
            current_chunk_content += "\n" + item['content'] + "\n"

    if current_chunk_content.strip():
        font_size_chunks.append({
            "header": current_chunk_header,
            "content": current_chunk_content.strip()
        })

    # --- 6. 2ì°¨ ì²­í‚¹ (ì‚¬ì´ì¦ˆ ê¸°ì¤€) ---
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=final_chunk_size,
        chunk_overlap=final_chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )

    final_documents = []
    for chapter in font_size_chunks:
        content = chapter['content']
        sub_chunks = recursive_splitter.split_text(content)
        for sub_chunk_content in sub_chunks:
            def is_valid_header(line):
                blacklist = {'â–¡', 'â€»', 'â€¢', '-', '*', 'Â·'}
                line_stripped = line.strip()
                if len(line_stripped) < 2:
                    return False
                if line_stripped in blacklist:
                    return False
                special_chars = set('`~!@#$%^&*()_+-=[]{}|;:\",./<>?Â·')
                total = len(line_stripped)
                if total == 0:
                    return False
                special_count = sum(1 for c in line_stripped if c in special_chars)
                if (special_count / total) > 0.6:
                    return False
                return True

            lines = [line.strip() for line in sub_chunk_content.split('\n') if line.strip()]
            valid_lines = [line for line in lines if is_valid_header(line)]
            if valid_lines:
                chunk_header = valid_lines[0]
            elif lines:
                chunk_header = lines[0]
            else:
                chunk_header = chapter['header']
            # --- í…Œì´ë¸” ì²­í¬ ë˜ëŠ” í…Œì´ë¸” í¬í•¨ ì²­í¬ëŠ” ì˜ˆì™¸ ì²˜ë¦¬ ---
            if "table" not in chunk_header.lower() and "|" not in sub_chunk_content:
                if is_high_special_char_ratio(sub_chunk_content):
                    continue
            final_metadata = metadata.copy()
            final_metadata['parent_header'] = chunk_header
            doc = Document(page_content=sub_chunk_content, metadata=final_metadata)
            final_documents.append(doc)

    return final_documents

def is_high_special_char_ratio(text: str, threshold: float = 0.6) -> bool:
    SPECIAL_CHARS = set(string.punctuation + '`~!@#$%^&*()_+-=[]{}|;:",./<>?Â·')
    # ì˜ˆì™¸ ë¬¸ì ì œì™¸
    EXCEPT_CHARS = set('â–¡â€»â€¢')
    text_for_check = ''.join(c for c in text if c not in EXCEPT_CHARS)
    total_length = len(text_for_check)
    if total_length == 0:
        return False
    special_char_count = sum(1 for c in text_for_check if c in SPECIAL_CHARS)
    return (special_char_count / total_length) > threshold