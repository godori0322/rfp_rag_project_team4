# document.py

import os
import re
import pandas as pd
import pdfplumber
import numpy as np
from typing import List, Dict, Any
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

from config import Config

# --- Helper Functions ---

def convert_table_to_markdown(table: List[List[str]]) -> str:
    """PDFplumberë¡œ ì¶”ì¶œëœ í…Œì´ë¸”(list of lists)ì„ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    markdown_table = ""
    if not table:
        return ""

    # í—¤ë” ìƒì„±
    header = [str(cell) if cell is not None else "" for cell in table[0]]
    markdown_table += "| " + " | ".join(header) + " |\n"
    
    # êµ¬ë¶„ì„  ìƒì„±
    markdown_table += "| " + " | ".join(["---"] * len(header)) + " |\n"
    
    # ë³¸ë¬¸ ìƒì„±
    for row in table[1:]:
        body = [str(cell) if cell is not None else "" for cell in row]
        markdown_table += "| " + " | ".join(body) + " |\n"
        
    return markdown_table

def clean_text_with_regex(text: str, patterns: List[str]) -> str:
    """ì£¼ì–´ì§„ ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´ë“¤ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì²­ì†Œí•©ë‹ˆë‹¤."""
    for pattern in patterns:
        text = re.sub(pattern, "", text)
    return text

# --- Main Functions ---

def load_documents():
    """CSV ë©”íƒ€ë°ì´í„°ì™€ PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    def ext(original_filename, ext='pdf'):
        base_filename, _ = os.path.splitext(original_filename)
        return f"{base_filename}.{ext}"

    df = pd.read_csv(os.path.join(Config.DATA_PATH, "data_list.csv"))
    
    date_columns = ['ê³µê°œ ì¼ì', 'ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼', 'ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼']
    for col in date_columns:
        df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%Y%m%d').fillna(0).astype(int)

    df = df.fillna('')

    all_docs = []
    for index, row in df.iterrows():
        print(f"[{index + 1}/{len(df)}] ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: {row['ì‚¬ì—…ëª…']}")
        metadata = {
            'rfp_number': row['ê³µê³  ë²ˆí˜¸'],
            'project_title': row['ì‚¬ì—…ëª…'],
            'budget_krw': row['ì‚¬ì—… ê¸ˆì•¡'],
            'agency': row['ë°œì£¼ ê¸°ê´€'],
            'publish_date': row['ê³µê°œ ì¼ì'],
            'bid_start_date': row['ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼'],
            'bid_end_date': row['ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼'],
            'summary': row['ì‚¬ì—… ìš”ì•½'],
            'filename': row['íŒŒì¼ëª…']
        }
        
        filepath = os.path.join(Config.PDF_PATH, ext(row['íŒŒì¼ëª…']))
        
        # ğŸ’¡ ê°œì„ ëœ chunk í•¨ìˆ˜ í˜¸ì¶œ
        # ì—¬ëŸ¬ íŒŒë¼ë¯¸í„°ë¥¼ ì‹¤í—˜í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        docs = chunk(
            filepath=filepath, 
            metadata=metadata,
            header_percentile=95, # ìƒìœ„ 1% í°íŠ¸ í¬ê¸°ë¥¼ í—¤ë”ë¡œ ê°„ì£¼
            final_chunk_size=2500, # ì²­í¬ ì‚¬ì´ì¦ˆ ì‹¤í—˜
            final_chunk_overlap=200  # ì²­í¬ ì˜¤ë²„ë© ì‹¤í—˜
        )
        all_docs.append(docs)

    print(f"ì´ {len(df)}ê°œì˜ ì›ë³¸ ë¬¸ì„œì—ì„œ {len(all_docs)}ê°œì˜ ì²­í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    return all_docs


def chunk(filepath: str, 
          metadata: dict, 
          header_percentile: int = 95, 
          final_chunk_size: int = 2500, 
          final_chunk_overlap: int = 200,
          noise_patterns: List[str] = None
         ) -> List[Document]:
    """
    ê°œì„ ëœ ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹ í•¨ìˆ˜.

    1. (ë…¸ì´ì¦ˆ ì œê±°) ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€ ì œê±°
    2. (í…Œì´ë¸” ì²˜ë¦¬) í…Œì´ë¸”ì„ Markdownìœ¼ë¡œ ë³€í™˜
    3. (ë™ì  í—¤ë” íƒì§€) í°íŠ¸ í¬ê¸° ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬ ë™ì ìœ¼ë¡œ í—¤ë” ì„ê³„ê°’ ì„¤ì •
    4. 1ì°¨ ì²­í‚¹: í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ì˜ 'ì±•í„°' ìƒì„±
    5. 2ì°¨ ì²­í‚¹: RecursiveCharacterTextSplitterë¡œ 'ì±•í„°'ë¥¼ ìµœì¢… í¬ê¸°ë¡œ ë¶„í• 
    """
    if noise_patterns is None:
        # ğŸ’¡ ì¼ë°˜ì ì¸ ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€, í˜ì´ì§€ ë²ˆí˜¸ ì œê±° íŒ¨í„´ (í•„ìš”ì‹œ ì¶”ê°€/ìˆ˜ì •)
        noise_patterns = [
            r"^\s*-\s*\d+\s*-\s*$",  # "- 1 -", "- 2 -" à®ªà¯‹à®©à¯à®± à®µà®Ÿà®¿à®µà®™à¯à®•à®³à¯
            r"^\s*\d+\s*$",         # í˜ì´ì§€ ë²ˆí˜¸ë§Œ ìˆëŠ” ê²½ìš°
            r"(?i)page\s*\d+\s*of\s*\d+", # "Page 1 of 10"
        ]

    page_items = [] # í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸”ì„ ìœ„ì¹˜ ì •ë³´ì™€ í•¨ê»˜ ì €ì¥
    all_font_sizes = []

    with pdfplumber.open(filepath) as pdf:
        for page_num, page in enumerate(pdf.pages):
            
            # --- 1. í…Œì´ë¸” ì¶”ì¶œ ë° ë³€í™˜ ---
            tables = page.extract_tables()
            for table in tables:
                md_table = convert_table_to_markdown(table)
                # í…Œì´ë¸”ì˜ y ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— í…ìŠ¤íŠ¸ì™€ ìˆœì„œëŒ€ë¡œ í•©ì¹˜ê¸° ìœ„í•¨)
                table_bbox = page.find_tables()[0].bbox
                page_items.append({'type': 'table', 'content': md_table, 'top': table_bbox[1], 'page': page_num})

            # --- 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë…¸ì´ì¦ˆ ì œê±° ---
            # í…Œì´ë¸” ì˜ì—­ì„ ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_without_tables = page.filter(lambda obj: obj["object_type"] == "char")
            
            # í°íŠ¸ ì‚¬ì´ì¦ˆ ìˆ˜ì§‘ ë° ì¤„ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
            current_line = ""
            current_top = -1
            line_size = 10 # ê¸°ë³¸ í°íŠ¸ í¬ê¸°

            for char in content_without_tables.chars:
                if current_top != char['top']:
                    if current_line.strip():
                        cleaned_line = clean_text_with_regex(current_line, noise_patterns)
                        if cleaned_line.strip():
                            page_items.append({'type': 'text', 'content': cleaned_line, 'size': line_size, 'top': current_top, 'page': page_num})
                    
                    current_line = ""
                    current_top = char['top']
                    line_size = char.get('size', 10)
                
                current_line += char['text']
                all_font_sizes.append(char.get('size', 10))
            
            # ë§ˆì§€ë§‰ ì¤„ ì¶”ê°€
            if current_line.strip():
                cleaned_line = clean_text_with_regex(current_line, noise_patterns)
                if cleaned_line.strip():
                    page_items.append({'type': 'text', 'content': cleaned_line, 'size': line_size, 'top': current_top, 'page': page_num})


    # --- 3. ë™ì  í—¤ë” ì„ê³„ê°’ ê³„ì‚° ---
    try:
        header_font_threshold = np.percentile(all_font_sizes, header_percentile)
    except IndexError: # ë¬¸ì„œì— í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš°
        header_font_threshold = 18 # ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´

    # --- 4. 1ì°¨ ì²­í‚¹ (í—¤ë” ê¸°ì¤€) ---
    # í˜ì´ì§€ ì•„ì´í…œë“¤ì„ í˜ì´ì§€ ë²ˆí˜¸ì™€ ìˆ˜ì§ ìœ„ì¹˜(top) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    page_items.sort(key=lambda x: (x['page'], x['top']))
    
    font_size_chunks = []
    current_chunk_content = ""
    current_chunk_header = f"ë¬¸ì„œ ì‹œì‘ ({os.path.basename(filepath)})"

    for item in page_items:
        if item['type'] == 'text':
            font_size = round(item.get('size', 0))
            text = item['content']
            
            if font_size >= header_font_threshold:
                if current_chunk_content.strip():
                    font_size_chunks.append({"header": current_chunk_header, "content": current_chunk_content.strip()})
                current_chunk_header = text
                current_chunk_content = ""
            else:
                current_chunk_content += text + "\n"
        
        elif item['type'] == 'table':
            current_chunk_content += "\n" + item['content'] + "\n"

    if current_chunk_content.strip():
        font_size_chunks.append({"header": current_chunk_header, "content": current_chunk_content.strip()})

    # --- 5. 2ì°¨ ì²­í‚¹ (ì‚¬ì´ì¦ˆ ê¸°ì¤€) ---
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=final_chunk_size,
        chunk_overlap=final_chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    final_documents = []
    for chapter in font_size_chunks:
        header = chapter['header']
        content = chapter['content']
        
        sub_chunks = recursive_splitter.split_text(content)
        for sub_chunk_content in sub_chunks:
            final_metadata = metadata.copy()
            final_metadata['parent_header'] = header
            doc = Document(page_content=sub_chunk_content, metadata=final_metadata)
            final_documents.append(doc)
            
    return final_documents