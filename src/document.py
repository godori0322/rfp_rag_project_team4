from config import Config
#from dotenv import load_dotenv, find_dotenv
#import fitz  # PyMuPDF
import pdfplumber
import os
import pandas as pd
from typing import List
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.schema import Document

def load_documents():
    def ext(original_filename, ext='pdf'):
        base_filename, _ = os.path.splitext(original_filename)
        return f"{base_filename}.{ext}"

    df = pd.read_csv(os.path.join(Config.DATA_PATH, "data_list.csv"))

    # NaN ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´í•˜ì—¬ ë©”íƒ€ë°ì´í„°ì— ë¬¸ì œê°€ ì—†ë„ë¡ ì²˜ë¦¬
    df = df.fillna('')

    doc_group = []

    for index, row in df.iterrows():
        # page_contentëŠ” 'í…ìŠ¤íŠ¸' ì»¬ëŸ¼ì˜ ë‚´ìš©ìœ¼ë¡œ ì„¤ì •
        print(f"ì´ê±´ {index + 1}ë²ˆì§¸ ë¬¸ì„œ: {row['ì‚¬ì—…ëª…']}")
        # if index > 3:
        #     break
        metadata = {
            'rfp_number': row['ê³µê³  ë²ˆí˜¸'],
            'project_title': row['ì‚¬ì—…ëª…'],
            'budget_krw': row['ì‚¬ì—… ê¸ˆì•¡'],
            'agency': row['ë°œì£¼ ê¸°ê´€'],
            'publish_date': row['ê³µê°œ ì¼ì'],
            'bid_start_date': row['ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼'],
            'bid_end_date': row['ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼'],
            'summary': row['ì‚¬ì—… ìš”ì•½'],
            'filename': row['íŒŒì¼ëª…']
        }

        # Document ê°ì²´ ìƒì„±
        docs = chunk(os.path.join(Config.PDF_PATH, ext(row['íŒŒì¼ëª…'])), metadata=metadata)
        doc_group.append(docs)
        """
        try:
            docs = load_documents(ext(row['íŒŒì¼ëª…']), metadata=metadata)
            documents.extend(docs)
        except Exception as e:
            print(f"Error loading document {row['íŒŒì¼ëª…']}: {e}")
            continue
        """
    print(f"ì´ {len(doc_group)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(doc_group[0])
    return doc_group


def chunk(filepath: str, metadata: dict, header_font_threshold: int = 18, final_chunk_size: int = 500, final_chunk_overlap: int = 50) -> List[Document]:

    """
    header_font_threshold: int = 18, 
    --> ğŸ¹ : ê°œì¸ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í•´ë´¤ì„ë•Œ ê°€ì¥ ì¢‹ì•˜ë˜ ì„ê³„ê°’ìœ¼ë¡œ ì ìš©í•´ë†¨ìŠµë‹ˆë‹¤.
    
    ### 1ì°¨ ìˆ˜ì •
    1. í°íŠ¸ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ êµ¬ì¡°ì ì¸ 'ì±•í„°'ë¥¼ ë¨¼ì € ë‚˜ëˆ•ë‹ˆë‹¤.
    2. ë‚´ìš©ì´ ê¸´ 'ì±•í„°'ëŠ” RecursiveCharacterTextSplitterë¡œ ë‹¤ì‹œ ì‘ê²Œ ë¶„í• í•©ë‹ˆë‹¤.
    
    ### 2ì°¨ ìˆ˜ì •
    1. pdfplumberë¥¼ ì‚¬ìš©í•´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (í…ìŠ¤íŠ¸ ì¶”ì¶œ)
    2. í°íŠ¸ í¬ê¸°ë¡œ ì„ê³„ê°’(Threshold) ê¸°ì¤€ìœ¼ë¡œ 1ì°¨ ì²­í‚¹ (ì±•í„° ìƒì„±)
    3. RecursiveCharacterTextSplitterë¡œ 2ì°¨ ì²­í‚¹
    
    
    """
    # --- ë¡œì»¬ í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---
    def reconstruct_lines_from_words(words: List[dict[str, any]]) -> List[dict[str, any]]:
        """pdfplumberì˜ ë‹¨ì–´(word) ëª©ë¡ì„ ì¤„(line) ë‹¨ìœ„ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
        lines = []
        if not words:
            return []
        
        current_line_words = [words[0]]
        for i in range(1, len(words)):
            # ê°™ì€ ì¤„ì— ìˆëŠ”ì§€ í™•ì¸ (ìˆ˜ì§ ìœ„ì¹˜ê°€ ê±°ì˜ ë™ì¼í•œ ê²½ìš°)
            if abs(words[i]['top'] - words[i-1]['top']) < 2:
                current_line_words.append(words[i])
            else:
                # ìƒˆ ì¤„ ì‹œì‘
                lines.append({
                    'text': ' '.join(w['text'] for w in current_line_words),
                    'size': current_line_words[0].get('size', 0) # ì²« ë‹¨ì–´ì˜ í¬ê¸°ë¥¼ ëŒ€í‘œë¡œ
                })
                current_line_words = [words[i]]
        
        # ë§ˆì§€ë§‰ ì¤„ ì¶”ê°€
        lines.append({
            'text': ' '.join(w['text'] for w in current_line_words),
            'size': current_line_words[0].get('size', 0)
        })
        return lines
    # --------------------------
    
    # pdfplumberë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
    reconstructed_lines = []
    with pdfplumber.open(filepath) as pdf:
        for page in pdf.pages:
            # extra_attrsë¡œ 'size'ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
            # extract_words -> ê° ë‹¨ì–´ì˜ í…ìŠ¤íŠ¸, ìœ„ì¹˜, í°íŠ¸ í¬ê¸°(size)
            
            words = page.extract_words(extra_attrs=["size", "fontname"])
            reconstructed_lines.extend(reconstruct_lines_from_words(words))

    # í°íŠ¸ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1ì°¨ ì²­í‚¹ (ì±•í„° ìƒì„±)
    font_size_chunks = []
    current_chunk_content = ""
    current_chunk_header = f"ë¬¸ì„œ ì‹œì‘ ({os.path.basename(filepath)})"

    for line in reconstructed_lines:
        font_size = round(line['size'])
        text = line['text']

        if font_size >= header_font_threshold:
            if current_chunk_content.strip():
                font_size_chunks.append({"header": current_chunk_header, "content": current_chunk_content.strip()})
            current_chunk_header = text
            current_chunk_content = ""
        else:
            current_chunk_content += text + "\n"
            
    if current_chunk_content.strip():
        font_size_chunks.append({"header": current_chunk_header, "content": current_chunk_content.strip()})

    # RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ 2ì°¨ ì²­í‚¹
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=final_chunk_size,
        chunk_overlap=final_chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    final_documents = []
    for chapter in font_size_chunks:
        header = chapter['header']
        content = chapter['content']
        
        # ë‚´ìš©ì´ ê¸´ ì±•í„°ë§Œ ë‹¤ì‹œ ë¶„í• 
        sub_chunks = recursive_splitter.split_text(content)
        for sub_chunk_content in sub_chunks:
            final_metadata = metadata.copy()
            final_metadata['parent_header'] = header
            doc = Document(page_content=sub_chunk_content, metadata=final_metadata)
            final_documents.append(doc)
        
    return final_documents