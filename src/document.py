from config import Config
#from dotenv import load_dotenv, find_dotenv
#import fitz  # PyMuPDF
import pdfplumber
import os
import pandas as pd
from typing import List
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.schema import Document
import string

def load_documents():
    def ext(original_filename, ext='pdf'):
        base_filename, _ = os.path.splitext(original_filename)
        return f"{base_filename}.{ext}"

    df = pd.read_csv(os.path.join(Config.DATA_PATH, "data_list.csv"))

    # ë‚ ì§œ ì»¬ëŸ¼ë“¤ì„ datetime íƒ€ì…ìœ¼ë¡œ í‘œì¤€í™”.
    # errors='coerce'ëŠ” ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê°’ì„ NaT(Not a Time)ìœ¼ë¡œ ì²˜ë¦¬.
    date_columns = ['ê³µê°œ ì¼ì', 'ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼', 'ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼']
    for col in date_columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')

    # NaN ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´í•˜ì—¬ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„°ì— ë¬¸ì œê°€ ì—†ë„ë¡ ì²˜ë¦¬.
    df = df.fillna('')
    doc_group = []
    annotations = []

    for index, row in df.iterrows():
        # í‘œì¤€í™”ëœ datetime ê°ì²´ë¥¼ "YYYY-MM-DD" í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜.
        # ë‚ ì§œ ì •ë³´ê°€ ì—†ëŠ”(NaT) ê²½ìš°ëŠ” ë¹ˆ ë¬¸ìì—´ ''ë¡œ ì²˜ë¦¬.
        publish_date_str = row['ê³µê°œ ì¼ì'].strftime('%Y-%m-%d') if pd.notna(row['ê³µê°œ ì¼ì']) else ''
        bid_start_date_str = row['ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼'].strftime('%Y-%m-%d') if pd.notna(row['ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼']) else ''
        bid_end_date_str = row['ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼'].strftime('%Y-%m-%d') if pd.notna(row['ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼']) else ''
        
        metadata = {
            'rfp_number': row['ê³µê³  ë²ˆí˜¸'],
            'project_title': row['ì‚¬ì—…ëª…'],
            'budget_krw': row['ì‚¬ì—… ê¸ˆì•¡'],
            'agency': row['ë°œì£¼ ê¸°ê´€'],
            
            # ìµœì¢… ë³€í™˜ëœ ë‚ ì§œ ë¬¸ìì—´ì„ ë©”íƒ€ë°ì´í„°ì— í• ë‹¹.
            'publish_date': publish_date_str,
            'bid_start_date': bid_start_date_str,
            'bid_end_date': bid_end_date_str,

            'summary': row['ì‚¬ì—… ìš”ì•½'],
            'filename': row['íŒŒì¼ëª…']
        }

        # Document ê°ì²´ ìƒì„±
        docs = chunk(os.path.join(Config.PDF_PATH, ext(row['íŒŒì¼ëª…'])), metadata=metadata)
        doc_group.append(docs)

        annotations.append(f"ì´ê±´ {index + 1}ë²ˆì§¸ ë¬¸ì„œ. ì´ ì²­í¬ê°¯ìˆ˜: {len(docs)}. {row['íŒŒì¼ëª…']}")
        print(annotations[len(annotations) - 1])
        
    print(f"ì´ {len(doc_group)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    for anno in annotations:
        print(anno)
    return doc_group


def chunk(filepath: str, metadata: dict, header_font_threshold: int = 18, final_chunk_size: int = 500, final_chunk_overlap: int = 50) -> List[Document]:

    """
    header_font_threshold: int = 18, 
    --> ğŸ¹ : ê°œì¸ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í•´ë´¤ì„ë•Œ ê°€ì¥ ì¢‹ì•˜ë˜ ì„ê³„ê°’ìœ¼ë¡œ ì ìš©í•´ë†¨ìŠµë‹ˆë‹¤.
    
    ### 1ì°¨ ìˆ˜ì •
    1. í°íŠ¸ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ êµ¬ì¡°ì ì¸ 'ì±•í„°'ë¥¼ ë¨¼ì € ë‚˜ëˆ•ë‹ˆë‹¤.
    2. ë‚´ìš©ì´ ê¸´ 'ì±•í„°'ëŠ” RecursiveCharacterTextSplitterë¡œ ë‹¤ì‹œ ì‘ê²Œ ë¶„í• í•©ë‹ˆë‹¤.
    
    ### 2ì°¨ ìˆ˜ì •
    1. pdfplumberë¥¼ ì‚¬ìš©í•´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (í…ìŠ¤íŠ¸ ì¶”ì¶œ)
    2. í°íŠ¸ í¬ê¸°ë¡œ ì„ê³„ê°’(Threshold) ê¸°ì¤€ìœ¼ë¡œ 1ì°¨ ì²­í‚¹ (ì±•í„° ìƒì„±)
    3. RecursiveCharacterTextSplitterë¡œ 2ì°¨ ì²­í‚¹
    
    
    """
    # --- ë¡œì»¬ í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---
    def reconstruct_lines_from_words(words: List[dict[str, any]]) -> List[dict[str, any]]:
        """pdfplumberì˜ ë‹¨ì–´(word) ëª©ë¡ì„ ì¤„(line) ë‹¨ìœ„ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
        lines = []
        if not words:
            return []

        current_line_words = [words[0]]
        for i in range(1, len(words)):
            # ê°™ì€ ì¤„ì— ìˆëŠ”ì§€ í™•ì¸ (ìˆ˜ì§ ìœ„ì¹˜ê°€ ê±°ì˜ ë™ì¼í•œ ê²½ìš°)
            if abs(words[i]['top'] - words[i-1]['top']) < 2:
                current_line_words.append(words[i])
            else:
                # ìƒˆ ì¤„ ì‹œì‘
                lines.append({
                    'text': ' '.join(w['text'] for w in current_line_words),
                    'size': current_line_words[0].get('size', 0) # ì²« ë‹¨ì–´ì˜ í¬ê¸°ë¥¼ ëŒ€í‘œë¡œ
                })
                current_line_words = [words[i]]

        # ë§ˆì§€ë§‰ ì¤„ ì¶”ê°€
        lines.append({
            'text': ' '.join(w['text'] for w in current_line_words),
            'size': current_line_words[0].get('size', 0)
        })
        return lines
    # --------------------------

    # pdfplumberë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
    reconstructed_lines = []
    try:
        with pdfplumber.open(filepath) as pdf:
            for page in pdf.pages:
                # extra_attrsë¡œ 'size'ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
                # extract_words -> ê° ë‹¨ì–´ì˜ í…ìŠ¤íŠ¸, ìœ„ì¹˜, í°íŠ¸ í¬ê¸°(size)

                words = page.extract_words(extra_attrs=["size", "fontname"])
                reconstructed_lines.extend(reconstruct_lines_from_words(words))
    except Exception as e:
        print(f"'{filepath}' íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return [] # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜


    # í°íŠ¸ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1ì°¨ ì²­í‚¹ (ì±•í„° ìƒì„±)
    font_size_chunks = []
    current_chunk_content = ""
    current_chunk_header = f"ë¬¸ì„œ ì‹œì‘ ({os.path.basename(filepath)})"

    for line in reconstructed_lines:
        font_size = round(line['size'])
        text = line['text']

        if font_size >= header_font_threshold:
            if current_chunk_content.strip():
                font_size_chunks.append({"header": current_chunk_header, "content": current_chunk_content.strip()})
            current_chunk_header = text
            current_chunk_content = ""
        else:
            current_chunk_content += text + "\n"

    if current_chunk_content.strip():
        font_size_chunks.append({"header": current_chunk_header, "content": current_chunk_content.strip()})

    # RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ 2ì°¨ ì²­í‚¹
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=final_chunk_size,
        chunk_overlap=final_chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    final_documents = []
    for chapter in font_size_chunks:
        header = chapter['header']
        content = chapter['content']
        
        # if 'ëª© ì°¨' == (" ".join(header.split()).strip()):
        #     print(f'### ë‹¤ìŒì€ ëª©ì°¨ë‚´ìš©ì´ë¼ ì œì™¸í•©ë‹ˆë‹¤ ((({content})))')
        #     continue

        # ë‚´ìš©ì´ ê¸´ ì±•í„°ë§Œ ë‹¤ì‹œ ë¶„í• 
        sub_chunks = recursive_splitter.split_text(content)
        for sub_chunk_content in sub_chunks:
            if is_high_special_char_ratio(sub_chunk_content):
                continue

            final_metadata = metadata.copy()
            final_metadata['parent_header'] = header
            doc = Document(page_content=sub_chunk_content, metadata=final_metadata)
            final_documents.append(doc)
        
    return final_documents


def is_high_special_char_ratio(text: str, threshold: float = 0.6) -> bool:
    SPECIAL_CHARS = set(string.punctuation + '`~!@#$%^&*()_+-=[]{}|;:",./<>?Â·') # íŠ¹ìˆ˜ë¬¸ì ì •ì˜ (êµ¬ë‘ì  ë° ê¸°íƒ€ ê¸°í˜¸)
    
    total_length = len(text)
    if total_length == 0:
        return False

    special_char_count = sum(1 for char in text if char in SPECIAL_CHARS)
    special_char_ratio = special_char_count / total_length

    if special_char_ratio > threshold:
        print(f"---- ì²­í¬ ì œì™¸ë¨ (íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ {special_char_ratio:.2f}) ----")
        # print(f"ì œì™¸ëœ ì²­í¬ ë‚´ìš©: {text}") # ë„ˆë¬´ ê¸¸ì–´ì„œ ì£¼ì„ ì²˜ë¦¬
    return special_char_ratio > threshold

