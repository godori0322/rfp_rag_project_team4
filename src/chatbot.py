import os
import json
import time
import numpy as np
from typing import List, Dict
from langsmith import Client
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage
from langchain.schema.output_parser import StrOutputParser
from langchain.schema import Document
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo
from langchain_core.runnables import RunnablePassthrough, RunnableBranch, RunnableLambda, chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.cross_encoders import HuggingFaceCrossEncoder # ë¦¬ë­ì»¤ ì„í¬íŠ¸
from langchain.retrievers import ContextualCompressionRetriever # ì••ì¶• ë¦¬íŠ¸ë¦¬ë²„ ì„í¬íŠ¸
from langchain.retrievers.document_compressors import CrossEncoderReranker # í¬ë¡œìŠ¤ ì¸ì½”ë” ì••ì¶•ê¸° ì„í¬íŠ¸
from langchain.callbacks.tracers import LangChainTracer

from config import Config, LangSmithConfig
from rag_graph import RAGCallbackHandler
from chain_router import ChainRouter


class Chatbot:
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.history_file = os.path.join(Config.HISTORY_PATH, f"{self.user_id}_history.json")

        os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

        # Validate LangSmith configuration
        LangSmithConfig.validate_config()
        
        # Initialize LangSmith client with explicit API key
        self.langsmith_client = Client(api_key=LangSmithConfig.LANGCHAIN_API_KEY, api_url=LangSmithConfig.LANGCHAIN_ENDPOINT)        
        self.tracer = LangChainTracer(project_name=LangSmithConfig.LANGCHAIN_PROJECT)
        self.initialize_components()
        self.history = self.load_history()
        self.router = ChainRouter(llm=self.llm,  retriever=self.retriever, self_query_retriever=self.self_query_retriever, vectorstore=self.vectorstore, tracer=self.tracer)
        self.chain = self.router.create_router_chain()
        self.rag_handler = RAGCallbackHandler()
        self.reranker_model = HuggingFaceCrossEncoder(model_name=Config.RERANK_MODEL)
        self.cross_reranker = CrossEncoderReranker(model=self.reranker_model, top_n=Config.TOP_K)

    def initialize_components(self):
        """Initializes the core components like LLM, embeddings, and retriever."""
        self.embeddings = OpenAIEmbeddings(model=Config.EMBEDDING_MODEL)
        self.vectorstore = Chroma(
            persist_directory=Config.VECTOR_DB_PATH,
            embedding_function=self.embeddings,
            collection_name=Config.RFP_COLLECTION
        )
        self.llm = ChatOpenAI(model=Config.LLM_MODEL, temperature=Config.TEMPERATURE)
        self.retriever = self.create_default_retriever()
        self.self_query_retriever = self.create_self_query_retriever()


    def create_default_retriever(self):
        base_retriever = self.vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k": Config.FIRST_TOP_K, "fetch_k": Config.FETCH_K, "lambda_mult": Config.LAMBDA_MULT}  
        )
        reranker_model = HuggingFaceCrossEncoder(model_name=Config.RERANK_MODEL)
        compressor = CrossEncoderReranker(model=reranker_model, top_n=Config.TOP_K)
        return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)


    def create_self_query_retriever(self):
        base_retriever = SelfQueryRetriever.from_llm(
            self.llm,
            self.vectorstore,
            document_contents="ì •ë¶€ ë° ê³µê³µê¸°ê´€ì—ì„œ ë°œì£¼í•˜ëŠ” RFP(ì œì•ˆìš”ì²­ì„œ)ì˜ ìƒì„¸ ë‚´ìš©. ì‚¬ì—… ê°œìš”, ì˜ˆì‚°, ê¸°ê°„, ì œì•ˆ ì¡°ê±´ ë“±ì„ í¬í•¨í•¨.",
            metadata_field_info=[
                AttributeInfo(name="rfp_number", type="string", description="ê³µê³  ë²ˆí˜¸"),
                AttributeInfo(name="project_title", type="string", description="ì‚¬ì—…ëª…"),
                AttributeInfo(name="budget_krw", type="integer", description="ì‚¬ì—… ê¸ˆì•¡"),
                AttributeInfo(name="agency", type="string", description="ë°œì£¼ ê¸°ê´€"),
                AttributeInfo(
                    name="publish_date", 
                    type="integer", 
                    description="ê³µê°œ ì¼ì. ê³µê³ ê°€ ê³µê°œëœ ë‚ ì§œ. YYYYMMDD í˜•ì‹ì˜ ì •ìˆ˜ê°’ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, '2024ë…„ 1ì›” 1ì¼'ì€ 20240101ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤."
                ),
                AttributeInfo(
                    name="bid_start_date", 
                    type="integer", 
                    description="ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼. ì…ì°° ì°¸ì—¬ê°€ ì‹œì‘ë˜ëŠ” ë‚ ì§œ. YYYYMMDD í˜•ì‹ì˜ ì •ìˆ˜ê°’ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, '2023ë…„ 12ì›” 25ì¼'ì€ 20231225ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤."
                ),
                AttributeInfo(
                    name="bid_end_date", 
                    type="integer", 
                    description="ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼. ì…ì°° ì°¸ì—¬ê°€ ë§ˆê°ë˜ëŠ” ë‚ ì§œ. YYYYMMDD í˜•ì‹ì˜ ì •ìˆ˜ê°’ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 'ì‘ë…„'ì´ë‚˜ '2023ë…„ ì´í›„' ê°™ì€ í‘œí˜„ë„ YYYYMMDD ì •ìˆ˜ í˜•ì‹ìœ¼ë¡œ ë°”ê¿”ì„œ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤."
                ),
                AttributeInfo(name="summary", type="string", description="ì‚¬ì—… ìš”ì•½"),
                AttributeInfo(name="filename", type="string", description="íŒŒì¼ëª…")
            ],
            search_kwargs={"k": Config.FIRST_TOP_K},
            verbose=True
        )
        reranker_model = HuggingFaceCrossEncoder(model_name=Config.RERANK_MODEL)
        compressor = CrossEncoderReranker(model=reranker_model, top_n=Config.TOP_K)
        return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)
        

    def find_documents(self, question):
        return self.router.find_documents(question)
    
    def find_contexts(self, docs):
        return self.router.find_contexts(docs)

    def load_history(self) -> list:
        if not os.path.exists(Config.HISTORY_PATH):
            os.makedirs(Config.HISTORY_PATH)

        if os.path.exists(self.history_file):
            with open(self.history_file, "r", encoding="utf-8") as f:
                history_json = json.load(f)
                return [
                    HumanMessage(content=msg['content']) if msg['type'] == 'human'
                    else AIMessage(content=msg['content']) for msg in history_json
                ]
        return []

    def save_history(self):
        with open(self.history_file, "w", encoding="utf-8") as f:
            history_json = [
                {'type': 'human', 'content': msg.content} if isinstance(msg, HumanMessage)
                else {'type': 'ai', 'content': msg.content} for msg in self.history
            ]
            json.dump(history_json, f, ensure_ascii=False, indent=4)

    def ask(self, question: str, save_history_flag: bool = True) -> dict:
        """
        Asks the chatbot a question and returns the answer and the context used.
        
        Returns:
            A dictionary containing 'answer' and 'context_docs'.
        """
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        start_time = time.time()

        inputs = {"input": question, "history": self.history}
        
        # invoke ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        result = self.chain.invoke(inputs)

        # --- ğŸ”¹ resultê°€ strì¸ì§€ dictì¸ì§€ í™•ì¸ ---
        if isinstance(result, dict):
            answer = result.get('answer', '')
            context_docs = result.get('docs', [])
        else:
            answer = str(result)  # strì´ë©´ ê·¸ëŒ€ë¡œ answer
            # context_docsë¥¼ ì¬í˜¸ì¶œí•˜ì§€ ì•Šê³  ì•ˆì „í•˜ê²Œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            context_docs = self.find_documents(question)

        # ì•ˆì „í•˜ê²Œ Document ë³€í™˜
        safe_docs = []
        for doc in context_docs:
            if isinstance(doc, Document):
                safe_docs.append(doc)
            elif isinstance(doc, str):
                safe_docs.append(Document(page_content=doc, metadata={}))
            else:
                safe_docs.append(Document(page_content=str(doc), metadata={}))

        # History ì—…ë°ì´íŠ¸
        if save_history_flag:
            self.history.extend([
                HumanMessage(content=question),
                AIMessage(content=answer),
            ])
            self.save_history()

        inference_time = time.time() - start_time
        return {
            "answer": answer,
            "context_docs": safe_docs,
            "inference_time": inference_time
        }