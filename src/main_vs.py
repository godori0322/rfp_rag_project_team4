from dotenv import load_dotenv, find_dotenv
import os
import pandas as pd
from typing import List
from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter
from langchain.schema import Document
from document import load_documents
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from typing import List

from config import Config

def create_vectorstore():
    load_dotenv(find_dotenv())

    if not Config.OPENAI_API_KEY or len(Config.OPENAI_API_KEY) < 20:
        print("OPENAI_API_KEYê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    if Config.IS_LOCAL:
        Config.to_local()
        embeddings = HuggingFaceEmbeddings(model_name=Config.EMBEDDING_MODEL, multi_process=True) # GPUê°€ 2ê°œì´ê¸° ë•Œë¬¸ì„
    else:
        embeddings = OpenAIEmbeddings(model=Config.EMBEDDING_MODEL, openai_api_key=Config.OPENAI_API_KEY)
    print(Config.EMBEDDING_MODEL, Config.LLM_MODEL, Config.VECTOR_DB_PATH)

    return
    
    print("ë¬¸ì„œ ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    doc_group = load_documents()
    print(f"ì´ {len(doc_group)}ê°œì˜ ë¬¸ì„œ ê·¸ë£¹ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")


    vector_store = Chroma(
        persist_directory=Config.VECTOR_DB_PATH, 
        embedding_function=embeddings, 
        collection_name=Config.RFP_COLLECTION
    )

    print("\nê¸°ì¡´ DBì˜ ë‚´ìš©ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    vector_store.delete_collection()
    # ë‹¤ì‹œ ì»¬ë ‰ì…˜ì„ ìƒì„±
    vector_store = Chroma(
        persist_directory=Config.VECTOR_DB_PATH, 
        embedding_function=embeddings, 
        collection_name=Config.RFP_COLLECTION
    )


    print("\në²¡í„° DBì— ë¬¸ì„œ ì¶”ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    cnt = 1
    batch_size = 100   # âœ… ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸° (ì¡°ì • ê°€ëŠ¥)

    for doc_chunks in doc_group:  # doc_group: List[List[Document]]
        for chunk in doc_chunks:
            if chunk.metadata is None:
                chunk.metadata = {}

        # ğŸ”¹ batch ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ì¶”ê°€
        for i in range(0, len(doc_chunks), batch_size):
            batch = doc_chunks[i:i+batch_size]
            vector_store.add_documents(batch)
            print(f" -> {cnt}ë²ˆ ë¬¸ì„œ ê·¸ë£¹ / batch {i//batch_size + 1} ì¶”ê°€ ì™„ë£Œ (ì²­í¬ {len(batch)}ê°œ)")
        
        cnt += 1

    print("\nëª¨ë“  ë¬¸ì„œì˜ ë²¡í„° ë³€í™˜ ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ì €ì¥ëœ ì´ ë¬¸ì„œ(ì²­í¬) ìˆ˜: {vector_store._collection.count()}")
    print(f"Vector Store ê²½ë¡œ: {Config.VECTOR_DB_PATH}")

if __name__ == "__main__":
    create_vectorstore()

